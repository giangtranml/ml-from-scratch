{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer: Attention is all you need\n",
    "\n",
    "This jupyter notebook is Tensorflow version implemented in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). The task is translating a source human-readable datetime to a target fixed datetime format **yyyy-mm-dd**, e.g: \"24th Aug 19\" -> \"2019-08-24\". Best way to start implement a model from scratch is using small dataset and non-complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "from faker import Faker\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import load_dataset_v2, preprocess_data, string_to_int, int_to_string, softmax\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 40000/40000 [00:01<00:00, 33482.49it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 40000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset_v2(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " ' ': 2,\n",
       " '.': 3,\n",
       " '/': 4,\n",
       " '0': 5,\n",
       " '1': 6,\n",
       " '2': 7,\n",
       " '3': 8,\n",
       " '4': 9,\n",
       " '5': 10,\n",
       " '6': 11,\n",
       " '7': 12,\n",
       " '8': 13,\n",
       " '9': 14,\n",
       " 'a': 15,\n",
       " 'b': 16,\n",
       " 'c': 17,\n",
       " 'd': 18,\n",
       " 'e': 19,\n",
       " 'f': 20,\n",
       " 'g': 21,\n",
       " 'h': 22,\n",
       " 'i': 23,\n",
       " 'j': 24,\n",
       " 'l': 25,\n",
       " 'm': 26,\n",
       " 'n': 27,\n",
       " 'o': 28,\n",
       " 'p': 29,\n",
       " 'r': 30,\n",
       " 's': 31,\n",
       " 't': 32,\n",
       " 'u': 33,\n",
       " 'v': 34,\n",
       " 'w': 35,\n",
       " 'y': 36}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#': 0,\n",
       " '-': 1,\n",
       " '0': 2,\n",
       " '1': 3,\n",
       " '2': 4,\n",
       " '3': 5,\n",
       " '4': 6,\n",
       " '5': 7,\n",
       " '6': 8,\n",
       " '7': 9,\n",
       " '8': 10,\n",
       " '9': 11}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (40000, 30)\n",
      "Y.shape: (40000, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "\n",
    "X, Y = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty+1)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model with Tensorflow.\n",
    "\n",
    "### Hyperparameter:\n",
    "\n",
    "$d_{model}$: dimension of word embeding, output of **Multi-head Attention** layer, output of **Feed Forward** layer.\n",
    "\n",
    "$d_k$: dimension of matrix Q, K\n",
    "\n",
    "$d_v$: dimension of matrix V\n",
    "\n",
    "$d_{ff}$: dimension of intermediate **Feed forward** layer\n",
    "\n",
    "$h$: number of heads at each block.\n",
    "\n",
    "\n",
    "### Positional Encoding:\n",
    "\n",
    "Since the Transformer model isn't sequential model like RNN and CNN. The computation is parallel over all input sentence flow from Embedding Layer, so we need to compute the relative or absolute position between the words. The author use non-trainable/fixed signusoid function:\n",
    "\n",
    "$$PE_{(pos, 2i)} = sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\mbox{this corresponding to the even indices}$$\n",
    "$$PE_{(pos, 2i+1)} = cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\mbox{this corresponding to the odd indices}$$\n",
    "\n",
    "where $pos$ is position in the sequence and $i$ is the dimension.\n",
    "\n",
    "\n",
    "### Scaled Dot-Product Attention:\n",
    "\n",
    "<img style=\"width:300px; height:300px\" src=\"https://i.imgur.com/HuXNlr0.png\" />\n",
    "\n",
    "$$Attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### (Encoder-Decoder) Multi-Head Attention:\n",
    "\n",
    "<img style=\"weight:300px; height:300px\" src=\"https://i.imgur.com/vgfOLR2.png\" />\n",
    "\n",
    "$$MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h)W^O$$\n",
    "$$\\mbox{where } head_i = Attention(Q, K, V)$$\n",
    "\n",
    "### Feed forward:\n",
    "\n",
    "$$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "### Encoder blocks:\n",
    "\n",
    "Each encoder block include 2 layers: **Multi-head Attention Mechanism** and **Position-wise Feed Forward**, respestively. Output at each layer use residual connection with its input followed by [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf): $LayerNorm(x + f(x))$\n",
    "\n",
    "### Decoder blocks:\n",
    "\n",
    "Each decoder block includes 3 layers: **Multi-head Attention Mechanism**, **Encoder-Decoder Multi-head Attention** and **Position-wise Feed Forward**. Same as **Encoder** blocks, output at each layer use residual connection with its input follow by Layer Normalization.\n",
    "\n",
    "<img src=\"https://i.imgur.com/1NUHvLi.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff, device):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_heads = num_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d_ff = d_ff\n",
    "        self.device = device\n",
    "        self.word_embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model).to(device)\n",
    "\n",
    "    def _init_structure(self, decoder_part=False):\n",
    "        assert not hasattr(self, \"pos_enc\"), \"The structure is initialized already.\"\n",
    "        self.pos_embed = torch.zeros(size=(self.seq_len, self.d_model), requires_grad=False, device=self.device)\n",
    "        for pos in range(self.seq_len):\n",
    "            for i in range(0, self.d_model//2):\n",
    "                self.pos_embed[pos, 2*i] = torch.sin(torch.Tensor([pos / (10000 ** (2*i/self.d_model))]))\n",
    "                self.pos_embed[pos, 2*i + 1] = torch.cos(torch.Tensor([pos / (10000 ** (2*i/self.d_model))]))\n",
    "        self.pos_embed = self.pos_embed.unsqueeze(0)\n",
    "\n",
    "        if decoder_part:\n",
    "            self.mask = [[0]*(i+1) + [-1e9]*(self.seq_len-(i+1)) for i in range(self.seq_len)]\n",
    "            self.mask = torch.tensor([self.mask], requires_grad=False).to(self.device)\n",
    "\n",
    "        for block_id in range(self.num_blocks):\n",
    "            # Self-attention sub-layer\n",
    "            setattr(self, \"Q\" + str(block_id), \n",
    "                    nn.Linear(in_features=self.d_model, out_features=self.d_k * self.num_heads).to(self.device))\n",
    "            setattr(self, \"K\" + str(block_id), \n",
    "                    nn.Linear(in_features=self.d_model, out_features=self.d_k * self.num_heads).to(self.device))\n",
    "            setattr(self, \"V\" + str(block_id), \n",
    "                    nn.Linear(in_features=self.d_model, out_features=self.d_v * self.num_heads).to(self.device))\n",
    "            # ---------------------------\n",
    "            \n",
    "            setattr(self, \"LN1\" + str(block_id), nn.LayerNorm(self.d_model).to(self.device))\n",
    "            \n",
    "            # Encoder-Decoder attention sub-layer\n",
    "            if decoder_part:\n",
    "                setattr(self, \"Qconn\" + str(block_id), \n",
    "                        nn.Linear(in_features=self.d_model, out_features=self.d_k * self.num_heads).to(self.device))\n",
    "                setattr(self, \"Kconn\" + str(block_id), \n",
    "                        nn.Linear(in_features=self.d_model, out_features=self.d_k * self.num_heads).to(self.device))\n",
    "                setattr(self, \"Vconn\" + str(block_id), \n",
    "                        nn.Linear(in_features=self.d_model, out_features=self.d_v * self.num_heads).to(self.device))\n",
    "                \n",
    "                setattr(self, \"LN2\" + str(block_id), nn.LayerNorm(self.d_v * self.num_heads).to(self.device))\n",
    "            # -----------------------------------\n",
    "            \n",
    "            # Layer multi-head attention output\n",
    "            setattr(self, \"O\" + str(block_id), \n",
    "                    nn.Linear(in_features=self.d_v * self.num_heads, out_features=self.d_model).to(self.device))\n",
    "            # Layer FNN 1\n",
    "            setattr(self, \"FNN1\" + str(block_id), \n",
    "                    nn.Linear(in_features=self.d_model, out_features=self.d_ff).to(self.device))\n",
    "            # Layer FNN 2\n",
    "            setattr(self, \"FNN2\" + str(block_id), \n",
    "                    nn.Linear(in_features=self.d_ff, out_features=self.d_model).to(self.device))\n",
    "            \n",
    "            setattr(self, \"LN3\" + str(block_id), nn.LayerNorm(self.d_model).to(self.device))\n",
    "\n",
    "    def _compute_multi_head_attention(self, Q, K, V, block_id, mask=False, connection_head=False):\n",
    "        if connection_head:\n",
    "            Q = getattr(self, \"Qconn\" + str(block_id))(Q)\n",
    "            K = getattr(self, \"Qconn\" + str(block_id))(K)\n",
    "            V = getattr(self, \"Qconn\" + str(block_id))(V)\n",
    "        else:\n",
    "            Q = getattr(self, \"Q\" + str(block_id))(Q)\n",
    "            K = getattr(self, \"Q\" + str(block_id))(K)\n",
    "            V = getattr(self, \"Q\" + str(block_id))(V)\n",
    "        QK = torch.einsum(\"ntk,nyk->nty\", Q, K)\n",
    "        if mask:\n",
    "            # apply mask to QK, prevent the affect of feature words to current word in decoder.\n",
    "            QK = QK + self.mask[:, :QK.shape[1], :QK.shape[2]]\n",
    "        QK = torch.softmax(QK/torch.sqrt(torch.Tensor([self.d_model]).to(self.device)), dim=-1)\n",
    "        atts = torch.einsum(\"nty,nyv->ntv\", QK, V)\n",
    "        O = getattr(self, \"O\" + str(block_id))(atts)\n",
    "        return O\n",
    "\n",
    "    def _compute_layer_norm(self, fX, X, name):\n",
    "        LN = getattr(self, name)(fX + X)\n",
    "        return LN\n",
    "\n",
    "    def _compute_fnn(self, X, block_id):\n",
    "        ffn1 = nn.functional.relu(getattr(self, \"FNN1\" + str(block_id))(X)) \n",
    "        ffn2 = getattr(self, \"FNN2\" + str(block_id))(ffn1)\n",
    "        return ffn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Transformer):\n",
    "\n",
    "    def __init__(self, num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff, device):\n",
    "        super(Encoder, self).__init__(num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff, device)\n",
    "        self._init_structure()\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = self.word_embed(X)\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        for block_id in range(self.num_blocks):\n",
    "            fx = self._compute_multi_head_attention(x, x, x, block_id)\n",
    "            x = self._compute_layer_norm(fx, x, \"LN1\" + str(block_id))\n",
    " \n",
    "            fx = self._compute_fnn(x, block_id)\n",
    "            x = self._compute_layer_norm(fx, x, \"LN3\" + str(block_id))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Transformer):\n",
    "    \n",
    "    def __init__(self, num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff, device):\n",
    "        super(Decoder, self).__init__(num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff, device)\n",
    "        self._init_structure(decoder_part=True)\n",
    "        self.O_last = nn.Linear(in_features=d_model, out_features=vocab_size).to(self.device)\n",
    "\n",
    "    def forward(self, X, encoder_output):\n",
    "        x = self.word_embed(X)\n",
    "        x = x + self.pos_embed[:, :x.shape[1], :]\n",
    "\n",
    "        for block_id in range(self.num_blocks):\n",
    "            fx = self._compute_multi_head_attention(x, x, x, block_id, mask=block_id==0)\n",
    "            x = self._compute_layer_norm(fx, x, \"LN1\" + str(block_id))\n",
    "\n",
    "            fx = self._compute_multi_head_attention(x, encoder_output, encoder_output, block_id, connection_head=True)\n",
    "            x = self._compute_layer_norm(fx, x, \"LN2\" + str(block_id))\n",
    "\n",
    "            fx = self._compute_fnn(x, block_id)\n",
    "            x = self._compute_layer_norm(fx, x, \"LN3\" + str(block_id))\n",
    "        \n",
    "        logits = self.O_last(x)\n",
    "        logits = torch.softmax(logits, -1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logits, target):\n",
    "    logits = logits.reshape(-1, logits.shape[-1])\n",
    "    target = target.reshape(-1)\n",
    "    return torch.nn.functional.cross_entropy(logits, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameter for Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BLOCKS = 2\n",
    "NUM_HEADS = 2\n",
    "DIMENSION_MODEL = 32\n",
    "DIMENSION_K = 16\n",
    "DIMENSION_V = 16\n",
    "DIMENSION_FF = 64\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(X).long().to(DEVICE)\n",
    "Y = torch.Tensor(Y).long().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(num_blocks=NUM_BLOCKS, num_heads=NUM_HEADS, vocab_size=len(human_vocab), seq_len=Tx, \n",
    "                  d_model=DIMENSION_MODEL, d_k=DIMENSION_K, d_v=DIMENSION_V, d_ff=DIMENSION_FF, device=DEVICE)\n",
    "\n",
    "decoder = Decoder(num_blocks=NUM_BLOCKS, num_heads=NUM_HEADS, vocab_size=len(machine_vocab), seq_len=Ty, \n",
    "                  d_model=DIMENSION_MODEL, d_k=DIMENSION_K, d_v=DIMENSION_V, d_ff=DIMENSION_FF, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "num_batches = X.shape[0]//batch_size if X.shape[0] % batch_size == 0 else X.shape[0]//batch_size + 1\n",
    "data = torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_update = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params_update, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8067448e7c564c659ba28cb5443c23c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=625, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1b1ada47a748089f111e45a6c01c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=625, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48803ad1ba6a470690363aeec0282196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=625, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8a1dc78c5d42be8db3495a60e69b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=625, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "E1018 23:18:50.813311  9204 ultratb.py:149] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-0fe2c3cbe7fe>\", line 22, in <module>\n",
      "    loss.backward()\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\", line 118, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 93, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    \n",
    "    data = data[torch.randperm(data.shape[0])]\n",
    "    \n",
    "    X, Y = data[:, :Tx], data[:, Tx:]\n",
    "    \n",
    "    pbar = tqdm.tqdm_notebook(range(0, num_batches), desc=\"Epoch \" + str(e+1))\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    for it in pbar:\n",
    "        loss = 0\n",
    "        start = it*batch_size\n",
    "        end = (it+1)*batch_size\n",
    "        \n",
    "        encoder_output = encoder(X[start:end])\n",
    "            \n",
    "        logits = decoder(Y[start:end, :-1], encoder_output)\n",
    "            \n",
    "        loss = loss_function(logits, Y[start:end, 1:])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "        train_loss += float(loss)\n",
    "        \n",
    "        pbar.set_description(\"Epoch %s - Training loss: %f\" % (e+1, (train_loss / (it+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "\n",
    "for example in EXAMPLES:\n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = torch.Tensor([source]).long().to(DEVICE)\n",
    "\n",
    "    encoder_output = encoder(source)\n",
    "    sentence = [machine_vocab[\"#\"]]\n",
    "\n",
    "    for t in range(Ty):\n",
    "        prediction = decoder(torch.Tensor([sentence]).long().to(DEVICE), encoder_output)\n",
    "        prediction = torch.argmax(prediction, dim=-1)\n",
    "        sentence.append(prediction[0][-1])\n",
    "\n",
    "    prediction = prediction.tolist()\n",
    "    #sequential_output = [inv_machine_vocab[s] for s in sentence[1:]]\n",
    "    parallel_output = [inv_machine_vocab[s] for s in prediction[0]]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"parallel output:\", ''.join(parallel_output))\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
